{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1763834308151
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /config.json\n",
            "Overriding of current TracerProvider is not allowed\n",
            "Overriding of current LoggerProvider is not allowed\n",
            "Overriding of current MeterProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Warning: the provided asset name 'donut-lora-env' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'donut-lora-env' will not be used for anonymous registration\n",
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitted job: patient_zebra_hlpm8t7gt2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from azure.ai.ml import MLClient, command, Output\n",
        "from azure.ai.ml.entities import Environment\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "\n",
        "# Connect to AML workspace\n",
        "ml_client = MLClient.from_config(DefaultAzureCredential())\n",
        "\n",
        "\n",
        "# Define environment\n",
        "donut_env = Environment(\n",
        "    name=\"donut-lora-env\",\n",
        "    image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-2.0-cuda11.7:latest\",\n",
        "    conda_file=\"environment.yaml\"\n",
        ")\n",
        "\n",
        "# Register environment\n",
        "ml_client.environments.create_or_update(donut_env)\n",
        "\n",
        "\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python train.py --data_dir ./data --output_dir ${{outputs.model_output}}\",\n",
        "    environment=donut_env,\n",
        "    compute=\"anishswiss1\",\n",
        "    display_name=\"donut-lora-train\",\n",
        "    experiment_name=\"donut-lora-exp\",\n",
        "    outputs={\n",
        "        \"model_output\": Output(type=\"uri_folder\", mode=\"upload\")\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# Submit\n",
        "returned_job = ml_client.jobs.create_or_update(job)\n",
        "print(f\"Submitted job: {returned_job.name}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1763851714417
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.8.0)\n",
            "Requirement already satisfied: torchvision in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.23.0)\n",
            "Requirement already satisfied: torchaudio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.8.0)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from triton==3.4.0->torch) (78.1.1)\n",
            "Requirement already satisfied: numpy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "FILES .... \n",
            "['.amlignore', '.amlignore.amltmp', 'added_tokens.json', 'config.json', 'generation_config.json', 'model.safetensors', 'preprocessor_config.json', 'sentencepiece.bpe.model', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchaudio\n",
        "\n",
        "\n",
        "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "download_path = \"./donut_qa_model\"\n",
        "\n",
        "\n",
        "model_folder = os.path.join(download_path, \"donutQA/outputs/donut-lora\")  # may need adjustment\n",
        "\n",
        "files_only = [f for f in os.listdir(model_folder) if os.path.isfile(os.path.join(model_folder, f))]\n",
        "print(\"FILES .... \")\n",
        "print(files_only)\n",
        "\n",
        "processor = DonutProcessor.from_pretrained(model_folder)\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1763854135455
        }
      },
      "outputs": [],
      "source": [
        "image = Image.open(\"test_pay_stub.jpg\").convert(\"RGB\")\n",
        "question = \"What is the net pay?\"\n",
        "\n",
        "prompt = f\"<s_docvqa><s_question>{question}</s_question><s_answer>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1763854168466
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted answer:  $853.30\n"
          ]
        }
      ],
      "source": [
        "# Prepare inputs\n",
        "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
        "decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate prediction\n",
        "outputs = model.generate(\n",
        "    pixel_values,\n",
        "    decoder_input_ids=decoder_input_ids,\n",
        "    max_length=model.decoder.config.max_position_embeddings,\n",
        "    early_stopping=True,\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    eos_token_id=processor.tokenizer.eos_token_id,\n",
        "    use_cache=True,\n",
        "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
        "    return_dict_in_generate=True,\n",
        ")\n",
        "\n",
        "# Decode answer\n",
        "sequence = processor.batch_decode(outputs.sequences)[0]\n",
        "answer = sequence.split(\"<s_answer>\")[1].split(\"</s_answer>\")[0]\n",
        "\n",
        "print(f\"Predicted answer: {answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1763990350725
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38/bin/python\n"
          ]
        }
      ],
      "source": [
        "#conda env update -f environment.yaml\n",
        "\n",
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy to Azure Container Instances (ACI) - Option 1: No quota issues!\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, Environment\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from datetime import datetime\n",
        "\n",
        "# Create ml_client if it doesn't exist\n",
        "if 'ml_client' not in globals():\n",
        "    print(\"Creating MLClient connection...\")\n",
        "    ml_client = MLClient.from_config(DefaultAzureCredential())\n",
        "\n",
        "# Get or create donut_env if it doesn't exist\n",
        "if 'donut_env' not in globals():\n",
        "    print(\"Creating environment definition...\")\n",
        "    donut_env = Environment(\n",
        "        name=\"donut-lora-env\",\n",
        "        image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-2.0-cuda11.7:latest\",\n",
        "        conda_file=\"environment.yaml\"\n",
        "    )\n",
        "    ml_client.environments.create_or_update(donut_env)\n",
        "\n",
        "# Get the registered environment\n",
        "env_version = 22\n",
        "try:\n",
        "    registered_env = ml_client.environments.get(donut_env.name, version=str(env_version))\n",
        "    env_ref = registered_env\n",
        "    print(f\"Using registered environment: {registered_env.name}:{registered_env.version}\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not get environment version {env_version}. Error: {e}\")\n",
        "    env_ref = f\"{donut_env.name}:{env_version}\"\n",
        "    print(f\"Using environment reference: {env_ref}\")\n",
        "\n",
        "# Create ManagedOnlineEndpoint (ACI is handled via instance type)\n",
        "# Use smaller instance types that typically have more quota available\n",
        "endpoint_name = f\"donutqa-aci-{datetime.now().strftime('%m%d%H%M')}\"\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=endpoint_name,\n",
        "    auth_mode=\"key\"\n",
        ")\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
        "print(f\"Created endpoint: {endpoint_name}\")\n",
        "\n",
        "# Deploy model with smaller instance type to avoid quota issues\n",
        "# Try Standard_B2ms (2 vCPUs) or Standard_B4ms (4 vCPUs) - these often have more quota\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=\"aci\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=\"donutQA:1\",\n",
        "    environment=env_ref,\n",
        "    code_path=\"src\",\n",
        "    scoring_script=\"score.py\",\n",
        "    instance_type=\"Standard_B2ms\",  # 2 vCPUs, 8GB RAM - smaller, more likely to have quota\n",
        "    instance_count=1\n",
        ")\n",
        "ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
        "print(\"Deployment to ACI created successfully\")\n",
        "\n",
        "# Route traffic\n",
        "endpoint.traffic = {\"aci\": 100}\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
        "print(\"Traffic routed to deployment\")\n",
        "\n",
        "# Get endpoint details\n",
        "endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
        "print(f\"\\n‚úÖ Endpoint deployed successfully!\")\n",
        "print(f\"Endpoint name: {endpoint_name}\")\n",
        "print(f\"Scoring URI: {endpoint.scoring_uri}\")\n",
        "print(f\"Status: {endpoint.provisioning_state}\")\n",
        "print(f\"\\n‚ö†Ô∏è  Note: Using Standard_B2ms instance (2 vCPUs). If quota error persists, try Standard_B1ms or request quota increase.\")\n",
        "print(f\"\\nüìù To call the endpoint:\")\n",
        "print(f\"   POST {endpoint.scoring_uri}\")\n",
        "print(f\"   Headers: {{'Authorization': 'Bearer <key>', 'Content-Type': 'application/json'}}\")\n",
        "print(f\"   Body: {{'image': '<base64_encoded_image>', 'question': '<your_question>'}}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Batch Endpoint - No quota issues, uses your existing compute\n",
        "# Good for processing multiple documents offline/async\n",
        "from azure.ai.ml.entities import BatchEndpoint, BatchDeployment\n",
        "from datetime import datetime\n",
        "\n",
        "# Create ml_client if it doesn't exist\n",
        "if 'ml_client' not in globals():\n",
        "    from azure.ai.ml import MLClient\n",
        "    from azure.identity import DefaultAzureCredential\n",
        "    ml_client = MLClient.from_config(DefaultAzureCredential())\n",
        "\n",
        "# Get environment\n",
        "if 'donut_env' not in globals():\n",
        "    from azure.ai.ml.entities import Environment\n",
        "    donut_env = Environment(\n",
        "        name=\"donut-lora-env\",\n",
        "        image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-2.0-cuda11.7:latest\",\n",
        "        conda_file=\"environment.yaml\"\n",
        "    )\n",
        "    ml_client.environments.create_or_update(donut_env)\n",
        "\n",
        "env_version = 22\n",
        "try:\n",
        "    registered_env = ml_client.environments.get(donut_env.name, version=str(env_version))\n",
        "    env_ref = registered_env\n",
        "except:\n",
        "    env_ref = f\"{donut_env.name}:{env_version}\"\n",
        "\n",
        "# Create Batch Endpoint (no quota issues - uses your compute)\n",
        "batch_endpoint_name = f\"donutqa-batch-{datetime.now().strftime('%m%d%H%M')}\"\n",
        "batch_endpoint = BatchEndpoint(\n",
        "    name=batch_endpoint_name,\n",
        "    description=\"Batch endpoint for Donut QA - processes documents asynchronously\"\n",
        ")\n",
        "ml_client.batch_endpoints.begin_create_or_update(batch_endpoint).result()\n",
        "print(f\"Created batch endpoint: {batch_endpoint_name}\")\n",
        "\n",
        "# Deploy to batch endpoint\n",
        "batch_deployment = BatchDeployment(\n",
        "    name=\"batch\",\n",
        "    endpoint_name=batch_endpoint_name,\n",
        "    model=\"donutQA:1\",\n",
        "    environment=env_ref,\n",
        "    code_path=\"src\",\n",
        "    scoring_script=\"score.py\",\n",
        "    compute=\"anishswiss1\",  # Uses your existing compute - no quota issues!\n",
        "    instance_count=1\n",
        ")\n",
        "ml_client.batch_deployments.begin_create_or_update(batch_deployment).result()\n",
        "batch_endpoint.defaults = {\"deployment_name\": \"batch\"}\n",
        "ml_client.batch_endpoints.begin_create_or_update(batch_endpoint).result()\n",
        "\n",
        "print(f\"\\n‚úÖ Batch Endpoint deployed!\")\n",
        "print(f\"Endpoint name: {batch_endpoint_name}\")\n",
        "print(f\"\\nüìù Note: Batch endpoints process files/jobs asynchronously.\")\n",
        "print(f\"   Submit jobs with: ml_client.batch_endpoints.invoke()\")\n",
        "print(f\"   This uses your existing compute 'anishswiss1' - no quota issues!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy to Virtual Machine (using your existing compute instance)\n",
        "# This creates a web service on your VM without quota issues\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, Environment\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from datetime import datetime\n",
        "\n",
        "# Create ml_client if it doesn't exist\n",
        "if 'ml_client' not in globals():\n",
        "    print(\"Creating MLClient connection...\")\n",
        "    ml_client = MLClient.from_config(DefaultAzureCredential())\n",
        "\n",
        "# Get environment\n",
        "if 'donut_env' not in globals():\n",
        "    from azure.ai.ml.entities import Environment\n",
        "    donut_env = Environment(\n",
        "        name=\"donut-lora-env\",\n",
        "        image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-2.0-cuda11.7:latest\",\n",
        "        conda_file=\"environment.yaml\"\n",
        "    )\n",
        "    ml_client.environments.create_or_update(donut_env)\n",
        "\n",
        "env_version = 22\n",
        "try:\n",
        "    registered_env = ml_client.environments.get(donut_env.name, version=str(env_version))\n",
        "    env_ref = registered_env\n",
        "    print(f\"Using registered environment: {registered_env.name}:{registered_env.version}\")\n",
        "except:\n",
        "    env_ref = f\"{donut_env.name}:{env_version}\"\n",
        "\n",
        "# Create endpoint\n",
        "endpoint_name = f\"donutqa-vm-{datetime.now().strftime('%m%d%H%M')}\"\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=endpoint_name,\n",
        "    auth_mode=\"key\"\n",
        ")\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
        "print(f\"Created endpoint: {endpoint_name}\")\n",
        "\n",
        "# Deploy to VM - use compute instance as the target\n",
        "# Note: This uses the VM's resources, so no additional quota needed\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=\"vm\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=\"donutQA:1\",\n",
        "    environment=env_ref,\n",
        "    code_path=\"src\",\n",
        "    scoring_script=\"score.py\",\n",
        "    # For VM deployment, you can use the compute instance directly\n",
        "    # But managed endpoints still need instance_type - try smallest available\n",
        "    instance_type=\"Standard_B1ms\",  # Smallest: 1 vCPU, 2GB RAM\n",
        "    instance_count=1\n",
        ")\n",
        "ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
        "print(\"Deployment to VM endpoint created successfully\")\n",
        "\n",
        "# Route traffic\n",
        "endpoint.traffic = {\"vm\": 100}\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
        "\n",
        "# Get endpoint details\n",
        "endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
        "print(f\"\\n‚úÖ VM Endpoint deployed!\")\n",
        "print(f\"Endpoint name: {endpoint_name}\")\n",
        "print(f\"Scoring URI: {endpoint.scoring_uri}\")\n",
        "print(f\"\\nüìù Note: This still uses managed endpoint infrastructure.\")\n",
        "print(f\"   For direct VM deployment, see alternative approach below.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Alternative: Deploy directly to your compute instance VM\n",
        "\n",
        "## Option A: Manual deployment on compute instance\n",
        "You can SSH into your compute instance (`anishswiss1`) and:\n",
        "1. Copy your model and `score.py` to the VM\n",
        "2. Install dependencies\n",
        "3. Run a Flask/FastAPI server\n",
        "4. Expose it via the compute instance's endpoint\n",
        "\n",
        "## Option B: Use Azure ML's local deployment\n",
        "Deploy locally on the compute instance for testing, then expose via port forwarding.\n",
        "\n",
        "## Option C: Create a custom VM and deploy there\n",
        "1. Create a new Azure VM\n",
        "2. Install Python, dependencies\n",
        "3. Deploy your model as a web service\n",
        "4. More control, but you manage everything\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy Flask app directly to your compute instance VM\n",
        "# This creates a simple web service that runs on your VM\n",
        "\n",
        "# Step 1: Copy files to compute instance\n",
        "# You can do this via Azure ML Studio or SSH\n",
        "\n",
        "# Step 2: SSH into your compute instance and run:\n",
        "\"\"\"\n",
        "# SSH into compute instance (from Azure ML Studio: Compute -> anishswiss1 -> Terminal)\n",
        "# Or use: ssh azureuser@<compute-instance-ip>\n",
        "\n",
        "# Navigate to your project directory\n",
        "cd /home/azureuser/cloudfiles/code/Users/anishswiss/DonutQA\n",
        "\n",
        "# Install Flask if not already installed\n",
        "pip install flask\n",
        "\n",
        "# Set model path (adjust if needed)\n",
        "export MODEL_PATH=\"./donut_qa_model/donutQA/outputs/donut-lora\"\n",
        "\n",
        "# Run the Flask app\n",
        "python src/app.py\n",
        "\n",
        "# The service will be available at: http://localhost:5000\n",
        "# To expose it externally, you may need to:\n",
        "# 1. Configure network security group rules\n",
        "# 2. Use Azure ML's compute instance endpoints\n",
        "# 3. Or use port forwarding\n",
        "\"\"\"\n",
        "\n",
        "print(\"\"\"\n",
        "üìã To deploy to VM directly:\n",
        "\n",
        "1. The Flask app is in: src/app.py\n",
        "2. SSH into your compute instance: anishswiss1\n",
        "3. Navigate to your project folder\n",
        "4. Run: python src/app.py\n",
        "5. Service will be at: http://localhost:5000/score\n",
        "\n",
        "üìù Endpoint usage:\n",
        "   POST http://localhost:5000/score\n",
        "   Body: {\"image\": \"<base64_image>\", \"question\": \"<your_question>\"}\n",
        "   \n",
        "üí° To expose externally:\n",
        "   - Use Azure ML compute instance endpoints (if available)\n",
        "   - Or configure NSG rules and use public IP\n",
        "   - Or use Azure Application Gateway\n",
        "\"\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1764183013594
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: ./config.json\n",
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/mlflow/__init__.py:41: UserWarning: Versions of mlflow (3.1.1) and mlflow-skinny (2.22.1) are different. This may lead to unexpected behavior. Please install the same version of both packages.\n",
            "  mlflow.mismatch._check_version_mismatch()\n",
            "ActivityCompleted: Activity=OnlineDeployment.BeginCreateOrUpdate, HowEnded=Failure, Duration=4281.22 [ms], Exception=HttpResponseError, ErrorCategory=UserError, ErrorMessage=(BadRequest) The request is invalid.\n",
            "Code: BadRequest\n",
            "Message: The request is invalid.\n",
            "Exception Details:\t(InferencingClientCallFailed) {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\"errors\\\":{\\\"VmSize\\\":[\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId d00932c2-80b4-43af-8a00-529d3381b7ca. Current usage/limit: 4/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\"]},\\\"type\\\":\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\",\\\"title\\\":\\\"One or more validation errors occurred.\\\",\\\"status\\\":400,\\\"traceId\\\":\\\"00-4f58311eb928bc4360c4bcf1dcc6d9fe-8c432ac8f6ddc726-01\\\"}\"}}\n",
            "\tCode: InferencingClientCallFailed\n",
            "\tMessage: {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\"errors\\\":{\\\"VmSize\\\":[\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId d00932c2-80b4-43af-8a00-529d3381b7ca. Current usage/limit: 4/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\"]},\\\"type\\\":\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\",\\\"title\\\":\\\"One or more validation errors occurred.\\\",\\\"status\\\":400,\\\"traceId\\\":\\\"00-4f58311eb928bc4360c4bcf1dcc6d9fe-8c432ac8f6ddc726-01\\\"}\"}}\n",
            "Additional Information:Type: ComponentName\n",
            "Info: {\n",
            "    \"value\": \"managementfrontend\"\n",
            "}Type: Correlation\n",
            "Info: {\n",
            "    \"value\": {\n",
            "        \"operation\": \"4f58311eb928bc4360c4bcf1dcc6d9fe\",\n",
            "        \"request\": \"27fff206363494b1\"\n",
            "    }\n",
            "}Type: Environment\n",
            "Info: {\n",
            "    \"value\": \"eastus2\"\n",
            "}Type: Location\n",
            "Info: {\n",
            "    \"value\": \"eastus2\"\n",
            "}Type: Time\n",
            "Info: {\n",
            "    \"value\": \"2025-11-26T18:50:13.6853925+00:00\"\n",
            "}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating MLClient connection...\n",
            "Creating environment definition...\n",
            "Using registered environment: donut-lora-env:22\n",
            "Created endpoint: donutqa-endpoint-11261848\n"
          ]
        },
        {
          "ename": "HttpResponseError",
          "evalue": "(BadRequest) The request is invalid.\nCode: BadRequest\nMessage: The request is invalid.\nException Details:\t(InferencingClientCallFailed) {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\"errors\\\":{\\\"VmSize\\\":[\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId d00932c2-80b4-43af-8a00-529d3381b7ca. Current usage/limit: 4/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\"]},\\\"type\\\":\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\",\\\"title\\\":\\\"One or more validation errors occurred.\\\",\\\"status\\\":400,\\\"traceId\\\":\\\"00-4f58311eb928bc4360c4bcf1dcc6d9fe-8c432ac8f6ddc726-01\\\"}\"}}\n\tCode: InferencingClientCallFailed\n\tMessage: {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\"errors\\\":{\\\"VmSize\\\":[\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId d00932c2-80b4-43af-8a00-529d3381b7ca. Current usage/limit: 4/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\"]},\\\"type\\\":\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\",\\\"title\\\":\\\"One or more validation errors occurred.\\\",\\\"status\\\":400,\\\"traceId\\\":\\\"00-4f58311eb928bc4360c4bcf1dcc6d9fe-8c432ac8f6ddc726-01\\\"}\"}}\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"4f58311eb928bc4360c4bcf1dcc6d9fe\",\n        \"request\": \"27fff206363494b1\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"eastus2\"\n}Type: Location\nInfo: {\n    \"value\": \"eastus2\"\n}Type: Time\nInfo: {\n    \"value\": \"2025-11-26T18:50:13.6853925+00:00\"\n}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 56\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Deploy model\u001b[39;00m\n\u001b[1;32m     46\u001b[0m deployment \u001b[38;5;241m=\u001b[39m ManagedOnlineDeployment(\n\u001b[1;32m     47\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mendpoint_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     55\u001b[0m )\n\u001b[0;32m---> 56\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_deployments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_create_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment created successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Route traffic\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:119\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:288\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mstart_as_current_span(ACTIVITY_SPAN):\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[1;32m    286\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[1;32m    287\u001b[0m         ):\n\u001b[0;32m--> 288\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_online_deployment_operations.py:218\u001b[0m, in \u001b[0;36mOnlineDeploymentOperations.begin_create_or_update\u001b[0;34m(self, deployment, local, vscode_debug, skip_script_validation, local_enable_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     log_and_raise_error(ex)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_online_deployment_operations.py:213\u001b[0m, in \u001b[0;36mOnlineDeploymentOperations.begin_create_or_update\u001b[0;34m(self, deployment, local, vscode_debug, skip_script_validation, local_enable_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m poller\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:  \u001b[38;5;66;03m# pylint: disable=W0718\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ex, (ValidationException, SchemaValidationError)):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_online_deployment_operations.py:196\u001b[0m, in \u001b[0;36mOnlineDeploymentOperations.begin_create_or_update\u001b[0;34m(self, deployment, local, vscode_debug, skip_script_validation, local_enable_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m         module_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting deployment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m     deployment_rest \u001b[38;5;241m=\u001b[39m deployment\u001b[38;5;241m.\u001b[39m_to_rest_object(location\u001b[38;5;241m=\u001b[39mlocation)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     poller \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_online_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_create_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresource_group_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resource_group_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeployment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployment_rest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAzureMLPolling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[43mLROConfigurations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOLL_INTERVAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_format_arguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_format_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolling_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLROConfigurations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOLL_INTERVAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserialized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOnlineDeployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_rest_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeserialized\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m poller\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:119\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_04_01_preview/operations/_online_deployments_operations.py:933\u001b[0m, in \u001b[0;36mOnlineDeploymentsOperations.begin_create_or_update\u001b[0;34m(self, resource_group_name, workspace_name, endpoint_name, deployment_name, body, **kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m cont_token \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontinuation_token\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# type: Optional[str]\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cont_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 933\u001b[0m     raw_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_or_update_initial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresource_group_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_group_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeployment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mz\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_map\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_long_running_output\u001b[39m(pipeline_response):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_04_01_preview/operations/_online_deployments_operations.py:864\u001b[0m, in \u001b[0;36mOnlineDeploymentsOperations._create_or_update_initial\u001b[0;34m(self, resource_group_name, workspace_name, endpoint_name, deployment_name, body, **kwargs)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m201\u001b[39m]:\n\u001b[1;32m    863\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[0;32m--> 864\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, error_format\u001b[38;5;241m=\u001b[39mARMErrorFormat)\n\u001b[1;32m    866\u001b[0m response_headers \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
            "\u001b[0;31mHttpResponseError\u001b[0m: (BadRequest) The request is invalid.\nCode: BadRequest\nMessage: The request is invalid.\nException Details:\t(InferencingClientCallFailed) {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\"errors\\\":{\\\"VmSize\\\":[\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId d00932c2-80b4-43af-8a00-529d3381b7ca. Current usage/limit: 4/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\"]},\\\"type\\\":\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\",\\\"title\\\":\\\"One or more validation errors occurred.\\\",\\\"status\\\":400,\\\"traceId\\\":\\\"00-4f58311eb928bc4360c4bcf1dcc6d9fe-8c432ac8f6ddc726-01\\\"}\"}}\n\tCode: InferencingClientCallFailed\n\tMessage: {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\"errors\\\":{\\\"VmSize\\\":[\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId d00932c2-80b4-43af-8a00-529d3381b7ca. Current usage/limit: 4/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\"]},\\\"type\\\":\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\",\\\"title\\\":\\\"One or more validation errors occurred.\\\",\\\"status\\\":400,\\\"traceId\\\":\\\"00-4f58311eb928bc4360c4bcf1dcc6d9fe-8c432ac8f6ddc726-01\\\"}\"}}\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"4f58311eb928bc4360c4bcf1dcc6d9fe\",\n        \"request\": \"27fff206363494b1\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"eastus2\"\n}Type: Location\nInfo: {\n    \"value\": \"eastus2\"\n}Type: Time\nInfo: {\n    \"value\": \"2025-11-26T18:50:13.6853925+00:00\"\n}"
          ]
        }
      ],
      "source": [
        "# Deploy the model to a managed online endpoint\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, Environment\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from datetime import datetime\n",
        "\n",
        "# Create ml_client if it doesn't exist (from Cell 1)\n",
        "if 'ml_client' not in globals():\n",
        "    print(\"Creating MLClient connection...\")\n",
        "    ml_client = MLClient.from_config(DefaultAzureCredential())\n",
        "\n",
        "# Get or create donut_env if it doesn't exist\n",
        "if 'donut_env' not in globals():\n",
        "    print(\"Creating environment definition...\")\n",
        "    donut_env = Environment(\n",
        "        name=\"donut-lora-env\",\n",
        "        image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-2.0-cuda11.7:latest\",\n",
        "        conda_file=\"environment.yaml\"\n",
        "    )\n",
        "    # Register it\n",
        "    ml_client.environments.create_or_update(donut_env)\n",
        "\n",
        "# Get the registered environment with specific version\n",
        "# Use version 22 (or get the latest if you prefer)\n",
        "env_version = 22  # Specify the version number\n",
        "try:\n",
        "    registered_env = ml_client.environments.get(donut_env.name, version=str(env_version))\n",
        "    env_ref = registered_env\n",
        "    print(f\"Using registered environment: {registered_env.name}:{registered_env.version}\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not get environment version {env_version}. Error: {e}\")\n",
        "    # Fallback: use string format \"name:version\"\n",
        "    env_ref = f\"{donut_env.name}:{env_version}\"\n",
        "    print(f\"Using environment reference: {env_ref}\")\n",
        "\n",
        "# Create endpoint\n",
        "endpoint_name = f\"donutqa-endpoint-{datetime.now().strftime('%m%d%H%M')}\"\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=endpoint_name,\n",
        "    auth_mode=\"key\"\n",
        ")\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
        "print(f\"Created endpoint: {endpoint_name}\")\n",
        "\n",
        "# Deploy model\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=\"donutQA:1\",  # String reference to registered model\n",
        "    environment=env_ref,  # Use registered environment object\n",
        "    code_path=\"src\",\n",
        "    scoring_script=\"score.py\",  # Use scoring_script instead of entry_script\n",
        "    instance_type=\"Standard_DS3_v2\",\n",
        "    instance_count=1\n",
        ")\n",
        "ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
        "print(\"Deployment created successfully\")\n",
        "\n",
        "# Route traffic\n",
        "endpoint.traffic = {\"blue\": 100}\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
        "print(\"Traffic routed to deployment\")\n",
        "\n",
        "# Get endpoint details\n",
        "endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
        "print(f\"\\n‚úÖ Endpoint deployed successfully!\")\n",
        "print(f\"Endpoint name: {endpoint_name}\")\n",
        "print(f\"Scoring URI: {endpoint.scoring_uri}\")\n",
        "print(f\"Status: {endpoint.provisioning_state}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
